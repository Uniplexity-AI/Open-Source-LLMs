{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "How to create their own AI language models (LLMs) using open-source technologies. While the provided text does not directly address building an AI system for web scraping and summarization, I will outline a general approach based on the contextual information available:\n",
        "\n",
        "Building an AI Assistant with Web Scraping and Summarization Capabilities:\n",
        "\n",
        "Setting Up Your Environment:\n",
        "1. **GitHub Repository**: Begin by exploring the provided GitHub Repo (implied in the content but not shown here). Look for more comprehensive guides that contain the necessary code, links, and instructions for constructing an AI assistant.\n",
        "\n",
        "2. **Lead Artsy Open-Source AI Models**: Check if the repository includes or refers to open-source LLMs such as `facebook/bart-large`, which has web scraping functionalities, though might not be integrated into the assistant yet.\n",
        "\n",
        "3. **Compute Resources**: If you're planning to handle web scraping, ensure that the operations done are heavy on CPU/GPU, which might require access to server-grade computing resources.\n",
        "\n",
        "4. **Interactive Tools**: Utilize libraries like OpenAI's `gpt-3` or similar, which provide conversational AI capabilities depending on the model's design and API use.\n",
        "\n",
        "Building the AI Assistant for Web Scraping and Summarization (Hypothetical Outline):\n",
        "\n",
        "1. **Choose Your LLM**: Depending on the platform you use (like Colab or Jupyter Notebook), you may need to load an LLM that supports both conversation and data processing techniques.\n",
        "\n",
        "2. **Data Collection and Preprocessing**: For web scraping, you would need to first handle the collection of data. Library like BeautifulSoup or Scrapy in Python can help scrape data from web pages.\n",
        "\n",
        "3. **Summarization Model**: The key to summarization is not in the LLM itself but in how you process the text data scraped from the web. Look into models like GPT-3's text summarization capabilities, although pure AI models may not directly perform this"
      ],
      "metadata": {
        "id": "gVmCMtdDFA6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install The Required Libaries**"
      ],
      "metadata": {
        "id": "wIOz8KQ6JPVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installation\n",
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RspSKUo4FE6Z",
        "outputId": "416d5c4c-c8a7-4712-b238-b221c43c2597"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#some of the urls used\n",
        "https://www.cbu.ac.zm/schoolsAndUnits/schoolofmedicine/adimission-requirements/\n",
        "https://lapansiindustries.com/blogs/mechatronics.html\n",
        "https://www.cbu.ac.zm/schoolsAndUnits/schoolofmedicine/adimission-requirements/\n",
        "https://www.cbu.ac.zm/schoolsAndUnits/schoolofmedicine/basic-sciences-department/\n",
        "https://www.cbu.ac.zm/schoolsAndUnits/schoolofengineering/courses-bachelor-of-mechanical-engineering-hons/\n",
        "https://medium.com/@skt7/build-your-first-ai-assistant-with-open-source-llm-on-google-colab-74bbb6fbca7b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "collapsed": true,
        "id": "3PLxwDmPFZYR",
        "outputId": "4b2e39f1-e07b-4f5b-bf01-743a3c305efe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-4-dcc3e3ada674>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-dcc3e3ada674>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    https://medium.com/@skt7/build-your-first-ai-assistant-with-open-source-llm-on-google-colab-74bbb6fbca7b\u001b[0m\n\u001b[0m                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Run the notebook enter your the url you what to scrap and give the ai instructions on what to do with the scrapped url**"
      ],
      "metadata": {
        "id": "qsXpsqHGJcds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Step 1: Function to scrape a webpage\n",
        "def scrape_webpage(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text_content = \"\\n\".join([p.get_text() for p in paragraphs])\n",
        "        return text_content\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Step 2: Function to send scraped content to the LLM\n",
        "def send_to_llm(content, question):\n",
        "    client = InferenceClient(\n",
        "        \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        token=\"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\",\n",
        "    )\n",
        "\n",
        "    full_prompt = f\"Here is the content:\\n{content}\\n\\nUser's question: {question}\"\n",
        "\n",
        "    # Send request to the LLM\n",
        "    response = client.chat_completion(\n",
        "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        max_tokens=500,\n",
        "        stream=False,  # We can set stream=True if we want incremental results\n",
        "    )\n",
        "\n",
        "    # Extract the response\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# Step 3: Main flow\n",
        "if __name__ == \"__main__\":\n",
        "    # Ask the user for a URL\n",
        "    url = input(\"Please enter the URL of the webpage to scrape: \")\n",
        "\n",
        "    # Scrape the webpage\n",
        "    scraped_content = scrape_webpage(url)\n",
        "\n",
        "    if scraped_content:\n",
        "        print(\"Webpage scraped successfully.\")\n",
        "        # Ask the user for their question\n",
        "        user_question = input(\"Please enter your question: \")\n",
        "\n",
        "        # Send the scraped content and user's question to the LLM\n",
        "        llm_response = send_to_llm(scraped_content, user_question)\n",
        "\n",
        "        # Print the response from the LLM\n",
        "        print(\"\\nLLM Response:\")\n",
        "        print(llm_response)\n",
        "    else:\n",
        "        print(\"Failed to retrieve webpage.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klsmRI9FFN0Z",
        "outputId": "5747d3a7-cfed-44a5-f136-bac10a1c113c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the URL of the webpage to scrape: https://medium.com/@skt7/build-your-first-ai-assistant-with-open-source-llm-on-google-colab-74bbb6fbca7b\n",
            "Webpage scraped successfully.\n",
            "Please enter your question: explain what is in the content and how i can use llm to build ai gents that web scrapps and makes summarizes\n",
            "\n",
            "LLM Response:\n",
            "The content you've provided seems to be an excerpt from a series of guides that aim to teach users how to create their own AI language models (LLMs) using open-source technologies. While the provided text does not directly address building an AI system for web scraping and summarization, I will outline a general approach based on the contextual information available:\n",
            "\n",
            "Building an AI Assistant with Web Scraping and Summarization Capabilities:\n",
            "\n",
            "Setting Up Your Environment:\n",
            "1. **GitHub Repository**: Begin by exploring the provided GitHub Repo (implied in the content but not shown here). Look for more comprehensive guides that contain the necessary code, links, and instructions for constructing an AI assistant.\n",
            "\n",
            "2. **Lead Artsy Open-Source AI Models**: Check if the repository includes or refers to open-source LLMs such as `facebook/bart-large`, which has web scraping functionalities, though might not be integrated into the assistant yet.\n",
            "\n",
            "3. **Compute Resources**: If you're planning to handle web scraping, ensure that the operations done are heavy on CPU/GPU, which might require access to server-grade computing resources.\n",
            "\n",
            "4. **Interactive Tools**: Utilize libraries like OpenAI's `gpt-3` or similar, which provide conversational AI capabilities depending on the model's design and API use.\n",
            "\n",
            "Building the AI Assistant for Web Scraping and Summarization (Hypothetical Outline):\n",
            "\n",
            "1. **Choose Your LLM**: Depending on the platform you use (like Colab or Jupyter Notebook), you may need to load an LLM that supports both conversation and data processing techniques.\n",
            "\n",
            "2. **Data Collection and Preprocessing**: For web scraping, you would need to first handle the collection of data. Library like BeautifulSoup or Scrapy in Python can help scrape data from web pages.\n",
            "\n",
            "3. **Summarization Model**: The key to summarization is not in the LLM itself but in how you process the text data scraped from the web. Look into models like GPT-3's text summarization capabilities, although pure AI models may not directly perform this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "API interface to hugging face model below"
      ],
      "metadata": {
        "id": "_9AHf1E2GFmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCBbsC11E8a6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    token=\"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\",\n",
        ")\n",
        "\n",
        "for message in client.chat_completion(\n",
        "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
        "\tmax_tokens=500,\n",
        "\tstream=True,\n",
        "):\n",
        "    print(message.choices[0].delta.content, end=\"\")"
      ]
    }
  ]
}